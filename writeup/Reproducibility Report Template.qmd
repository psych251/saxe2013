---
title: "Reproducibility Report for Saxe, McClelland & Ganguli (2013, Proceedings of Cognitive Science Society)"
author: "S. Jerome Han (sjeromeh@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

### Justification for choice of study

My research interests are best described by two questions: 

1. What is it that artificial neural networks actually learn from data, and how do they do so? 
2. What other neural processes do we need to understand in order to account for human-like reasoning and problem solving in noisy, few-shot settings?

The first question is directly inspired by the work of Saxe et al., and I hope that the methods and intuitions that underlie their work will help me pursue a research program that examines the learning dynamics and knowledge representations of contemporary deep learning systems such as large language models.

The second question requires an intimate familiarity with neural network theory. In reproducing Saxe et al., I also hope to fortify my ability to understand and build upon the mathematics that underlies deep learning.

Thus, reproducing this study will both expand my methodological toolkit and provide me with the confidence to pursue a flavor of research that I currently still find a little intimidating.

### Anticipated challenges

For me, this project is inherently challenging because I lack a rigorous background in mathematics. No code is available online, so I will have to complete the implementation based on just the paper. I am also cautious about the feasibility of condensing its intuitions into a 5 minute presentation that is accessible to other students who might also lack existing background. However, it is precisely these challenges that motivate me to take on this project, and I see no significant technical challenges that might inhibit its success.

### Links

Project repository (on Github): https://github.com/psych251/saxe2013

Original paper (as hosted in your repo): https://github.com/psych251/saxe2013/blob/main/original_paper/SaxeMcCGanguli13CogSciProc.pdf

## Methods

### Description of the steps required to reproduce the results

The goal of this project is to reproduce the figures presented by Saxe et al. This includes figures 2, 3, 4 and 5, with most of the work revolving around the final section of the paper, 'The singular values and vectors of hierarchically generated data'.

To reproduce figure 2, I will recreate the toy category dataset that Saxe et al. use. This is trivial because the dataset is a simple 5x4 binary feature matrix that I can copy from the paper. I will then run SVD on this dataset and write code that turns the resulting matrices into a compelling graphic.

Figures 3, 4 and 5 require artificial data that is generated by a simple generative model of hierarchical data. I will implement this model using Python or R, following Saxe et al.'s description in the section 'Hierarchical feature vectors from a branching diffusion process'.

Figure 3 also requires me to implement the simple one layer deep linear network that Saxe et al. use in their experiments. This will likely be achieved using PyTorch, although it may also be possible to use the keras package for R if I decide to keep everything in R.

To fully reproduce figures 3, 4 and 5 I will follow Saxe et al.'s figure and main text descriptions to apply SVD to the generated datasets and the training runs of the deep linear network. This will then be paired with a figure generating pipeline that (hopefully) precisely reproduces their original figures.

### Differences from original study

It is not clear what the computing environment of the original implementation was. The implementation itself does not exist publicly. However, I do not foresee the specific computing environment mattering for reproducibility; the results here should be reproducible regardless of if I use PyTorch and Python or Keras and R.

## Project Progress Check 1

### Measure of success

This project might be a bit unique because Saxe et al. do not use any statistical tests in their paper. This is because it first focuses on mathematical theory before moving on to simulations that demonstrate their theory in an empirical setting. Statistical tests are therefore somewhat redundant and also difficult to formulate. Thus, my measure of success is binary for now. Specifically: can I reproduce Saxe et al.'s figures in a way that will allow me to compellingly present their paper and theory to the 251 class?

Figures 2 and 4 should be straightforward to reproduce, but an additional meta-level measure of success for me is whether I can recreate these figure in an aesthetic sense using code alone. I've previously tended to produce/edit more complex figures using design software, which is not the best for reproducibility, so I'd like to further develop my fancy figure coding abilities here.

Figure 3 illustrates the 'close agreement' between a theoretically predicted time course and several simulations. I may potentially explore how 'close agreement' can be better quantified, eg. using a Kolmogorov-Smirnov test, but for now I am satisfied with the intuitive measurement of 'does the reproduced figure mostly look like the one in the original paper and align with theoretical intuitions'. Given that these are random simulations, I expect some variation but not enough to derail the point of the figure.

Figure 5 compares theoretical and simulated singular values obtained from running SVD on Saxe et al.'s hierarchically generated data. I am currently drawing a blank on what kind of statistical test may potentially be appropriate to evaluate the fit between theory and simulation, so I will leave this for a later stage (if at all). For now I am also satisfied with the intuitive visual measurement.


### Pipeline progress

In a separate R file, I have a Keras implementation of the deep linear network that Saxe et al. use in their study. I have also recreated the toy category dataset that Saxe et al. use for figure 2. This dataset is saved in a small csv file. Finally, I have implemented the probabilistic generative model of hierarchical data that is needed for figures 3, 4 and 5. I should now be ready to run the full analysis and begin generating preliminary figures.

## Results

### Data preparation

Data preparation following the analysis plan.

```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Key analysis

The analyses as specified in the analysis plan.

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them). None of these need to be long.
